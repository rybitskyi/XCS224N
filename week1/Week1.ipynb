{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun: good\n",
      "noun: good, goodness\n",
      "noun: good, goodness\n",
      "noun: commodity, trade_good, good\n",
      "adj: good\n",
      "adj (s): full, good\n",
      "adj: good\n",
      "adj (s): estimable, good, honorable, respectable\n",
      "adj (s): beneficial, good\n",
      "adj (s): good\n",
      "adj (s): good, just, upright\n",
      "adj (s): adept, expert, good, practiced, proficient, skillful, skilful\n",
      "adj (s): good\n",
      "adj (s): dear, good, near\n",
      "adj (s): dependable, good, safe, secure\n",
      "adj (s): good, right, ripe\n",
      "adj (s): good, well\n",
      "adj (s): effective, good, in_effect, in_force\n",
      "adj (s): good\n",
      "adj (s): good, serious\n",
      "adj (s): good, sound\n",
      "adj (s): good, salutary\n",
      "adj (s): good, honest\n",
      "adj (s): good, undecomposed, unspoiled, unspoilt\n",
      "adj (s): good\n",
      "adv: well, good\n",
      "adv: thoroughly, soundly, good\n"
     ]
    }
   ],
   "source": [
    "# e.g. synonym sets containing “good”:\n",
    "poses = { 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}\n",
    "for synset in wn.synsets(\"good\"):\n",
    "    print(\"{}: {}\".format(poses[synset.pos()],\n",
    "    \", \".join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. hypernyms of “panda”:\n",
    "panda = wn.synset(\"panda.n.01\")\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['START', 'All', 'that', 'glitters', \"isn't\", 'gold', 'END'],\n",
       " ['START', \"All's\", 'well', 'that', 'ends', 'well', 'END'],\n",
       " ['START', 'All', 'that', 'that', \"isn't\", 'glitters', 'END']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"START All that glitters isn't gold END\".split(\" \"), \n",
    "          \"START All's well that ends well END\".split(\" \"),\n",
    "         \"START All that that isn't glitters END\".split(\" \"),]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-1282ead1c72e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorpus_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_corpus_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistinct_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mword2Ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = []\n",
    "    num_corpus_words = -1\n",
    "    \n",
    "### SOLUTION BEGIN\n",
    "    words = set()\n",
    "    for doc in corpus:\n",
    "        words.update([word for word in doc])\n",
    "    corpus_words = sorted(words)\n",
    "    num_corpus_words = len(words)\n",
    "### SOLUTION END\n",
    "\n",
    "    return corpus_words, num_corpus_words\n",
    "\n",
    "words, num_words = distinct_words(corpus)\n",
    "word2Ind = {}\n",
    "for i, word in enumerate(words):\n",
    "    word2Ind[word] = i\n",
    "print(word2Ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 8, 5, 7, 6, 2],\n",
       "       [3, 1, 9, 8, 4, 9, 2],\n",
       "       [3, 0, 8, 8, 7, 5, 2]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_index = np.array([[word2Ind[word] for word in doc] for doc in corpus])\n",
    "corpus_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 2]), array([2, 3, 2, 3]))\n",
      "<class 'tuple'>\n",
      "doc_index: 0 wi: 2\n",
      "doc_index: 1 wi: 3\n",
      "doc_index: 2 wi: 2\n",
      "doc_index: 2 wi: 3\n"
     ]
    }
   ],
   "source": [
    "found_word_positions = np.where(corpus_index == 8)\n",
    "print(found_word_positions)\n",
    "print(type(found_word_positions))\n",
    "for t in np.array(found_word_positions).T:\n",
    "    doc_index = t[0]\n",
    "    wi = t[1]\n",
    "    print(f\"doc_index: {doc_index} wi: {wi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wi: 0\n",
      "wi: 0\n",
      "wi: 0\n"
     ]
    }
   ],
   "source": [
    "a = 'START'\n",
    "for doc in corpus:\n",
    "    for wi, ww in enumerate(doc):\n",
    "        if ww == a:\n",
    "            print(f\"wi: {wi}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3,)\n",
      "1 2 3\n",
      "[5 2 3]\n",
      "(2, 3)\n",
      "1 2 4\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])   # Create a rank 1 array\n",
    "print(type(a))            # Prints \"<class 'numpy.ndarray'>\"\n",
    "print(a.shape)            # Prints \"(3,)\"\n",
    "print(a[0], a[1], a[2])   # Prints \"1 2 3\"\n",
    "a[0] = 5                  # Change an element of the array\n",
    "print(a)                  # Prints \"[5, 2, 3]\"\n",
    "\n",
    "b = np.array([[1,2,3],[4,5,6]])    # Create a rank 2 array\n",
    "print(b.shape)                     # Prints \"(2, 3)\"\n",
    "print(b[0, 0], b[0, 1], b[1, 0])   # Prints \"1 2 4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape (3, 2)\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "[[False False]\n",
      " [ True  True]\n",
      " [ True  True]]\n",
      "[3 4 5 6]\n",
      "[3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2], [3, 4], [5, 6]])\n",
    "print(f\"a.shape {a.shape}\")\n",
    "print(a)\n",
    "print(\"\")\n",
    "\n",
    "bool_idx = (a > 2)   # Find the elements of a that are bigger than 2;\n",
    "                     # this returns a numpy array of Booleans of the same\n",
    "                     # shape as a, where each slot of bool_idx tells\n",
    "                     # whether that element of a is > 2.\n",
    "\n",
    "print(bool_idx)      # Prints \"[[False False]\n",
    "                     #          [ True  True]\n",
    "                     #          [ True  True]]\"\n",
    "\n",
    "# We use boolean array indexing to construct a rank 1 array\n",
    "# consisting of the elements of a corresponding to the True values\n",
    "# of bool_idx\n",
    "print(a[bool_idx])  # Prints \"[3 4 5 6]\"\n",
    "\n",
    "# We can do all of the above in a single concise statement:\n",
    "print(a[a > 2])     # Prints \"[3 4 5 6]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple of arrays returned :  (array([ 4,  7, 11]),)\n",
      "Elements with value 15 exists at following indices\n",
      "[ 4  7 11]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([11, 12, 13, 14, 15, 16, 17, 15, 11, 12, 14, 15, 16, 17])\n",
    "# Get the index of elements with value 15\n",
    "result = np.where(arr == 15)\n",
    "print('Tuple of arrays returned : ', result)\n",
    "print(\"Elements with value 15 exists at following indices\", result[0], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # \"START All that glitters isn't gold END\"\n",
    "    # \"START All's well that ends well END\"\n",
    "    # print(word_context(corpus[0], 0, 4))\n",
    "\n",
    "    # str = \"START All that glitters isn't gold END\".split(\" \")\n",
    "    # print(f\"f0: {word_context(str, 0, 2)}\")\n",
    "    # print(f\"f1: {word_context(str, 1, 2)}\")\n",
    "    # print(f\"f2: {word_context(str, 2, 2)}\")\n",
    "    # print(f\"f3: {word_context(str, 3, 2)}\")\n",
    "    # print(f\"f4: {word_context(str, 4, 2)}\")\n",
    "    # print(f\"f5: {word_context(str, 5, 2)}\")\n",
    "    # print(f\"f6: {word_context(str, 6, 2)}\")\n",
    "    # return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/yrybitskyi/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "def read_corpus(category=\"crude\"):\n",
    "    \"\"\" Read files from the specified Reuter's category.\n",
    "        Params:\n",
    "            category (string): category name\n",
    "        Return:\n",
    "            list of lists, with words from each of the processed files\n",
    "    \"\"\"\n",
    "    files = reuters.fileids(category)\n",
    "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_corpus = read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = []\n",
    "    num_corpus_words = -1\n",
    "    \n",
    "### SOLUTION BEGIN\n",
    "    words = set()\n",
    "    def is_word(s):\n",
    "        s = s.replace(\"'\", \"\")\n",
    "        if len(s) == 0:\n",
    "            return False\n",
    "        if s.isdigit():\n",
    "            return False\n",
    "        return s.isalnum()\n",
    "\n",
    "    for doc in corpus:\n",
    "        words.update([word for word in doc if is_word(word)])\n",
    "\n",
    "    corpus_words = sorted(words)\n",
    "    num_corpus_words = len(words)\n",
    "### SOLUTION END\n",
    "\n",
    "    return corpus_words, num_corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
    "    \n",
    "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
    "              number of co-occurring words.\n",
    "              \n",
    "              For example, if we take the document \"START All that glitters is not gold END\" with window size of 4,\n",
    "              \"All\" will co-occur with \"START\", \"that\", \"glitters\", \"is\", and \"not\".\n",
    "    \n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)):\n",
    "                Co-occurrence matrix of word counts. \n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)\n",
    "    M = None\n",
    "    word2Ind = {}\n",
    "\n",
    "    ### SOLUTION BEGIN\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        word2Ind[word] = i\n",
    "\n",
    "    def relu(x):\n",
    "        if x > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # + 1. remove second is_word function\n",
    "    # 2. cache word existing\n",
    "    # 3. skip j rows that are not used in j rows\n",
    "    # + 4. fix 2pac\n",
    "\n",
    "\n",
    "    # def is_word(s):\n",
    "    #     s = s.replace(\"'\", \"\")\n",
    "    #     return len(s) > 1 and s[0].isalpha() and s.isalnum()\n",
    "\n",
    "    max_doc_length = 0\n",
    "    for doc in corpus:\n",
    "        doc_length = len(doc)\n",
    "        if doc_length > max_doc_length:\n",
    "            max_doc_length = doc_length\n",
    "\n",
    "\n",
    "    corpus_index = np.zeros((len(corpus), max_doc_length))\n",
    "    skipped_words = set()\n",
    "    for i, doc in enumerate(corpus):\n",
    "        for j, word in enumerate(doc):\n",
    "            w_idx = word2Ind.get(word)\n",
    "            if w_idx is not None:\n",
    "                corpus_index[i, j] = w_idx\n",
    "            else:\n",
    "                skipped_words.add(word)\n",
    "\n",
    "    print(f\"skipped_words : {sorted(skipped_words)}\")\n",
    "\n",
    "    print(f\"corpus_index.shape: {corpus_index.shape}\")\n",
    "    def word_context(doc_index, word_index, wd_size):\n",
    "        word_context_map = {}\n",
    "\n",
    "        doc = corpus_index[doc_index]\n",
    "        left_context = doc[relu(word_index - wd_size) : word_index]\n",
    "        right_context = doc[word_index + 1 : word_index + wd_size + 1]\n",
    "\n",
    "        for w in left_context:\n",
    "            cnt = word_context_map.get(w, 0)\n",
    "            word_context_map[w] = cnt + 1\n",
    "        for w in right_context:\n",
    "            cnt = word_context_map.get(w, 0)\n",
    "            word_context_map[w] = cnt + 1\n",
    "\n",
    "        return word_context_map\n",
    "\n",
    "    # M = np.zeros((num_words, num_words))\n",
    "    M = np.empty((num_words, num_words))\n",
    "    M[:] = np.nan\n",
    "\n",
    "    # Cache all words in corpus\n",
    "    # word_presence = np.empty(num_words)\n",
    "    word_presence = [None] * num_words\n",
    "    for i in range(num_words):\n",
    "        word_presence[i] = np.array(np.where(corpus_index == i)).T\n",
    "\n",
    "\n",
    "    #\n",
    "    for i in range(num_words):\n",
    "        # a = words[i]\n",
    "        # print(f\"{i} '{a}' from {num_words}\")\n",
    "        print(f\"{i} from {num_words}\")\n",
    "        i_word_presence = word_presence[i]\n",
    "        for j in range(num_words):\n",
    "            if i == j:\n",
    "                M[i, j] = 0\n",
    "                continue\n",
    "            if np.isnan(M[j, i]):\n",
    "                cnt = 0\n",
    "                # a_idx = word2Ind[a]\n",
    "                # found_word_positions = np.where(corpus_index == a_idx)\n",
    "                # for t in np.array(found_word_positions).T:\n",
    "                for t in i_word_presence:\n",
    "                    doc_index = t[0]\n",
    "                    wi = t[1]\n",
    "                    word_context_map = word_context(doc_index, wi, window_size)\n",
    "                    cnt = cnt + word_context_map.get(j, 0)\n",
    "\n",
    "                M[i, j] = cnt\n",
    "            else:\n",
    "                M[i, j] = M[j, i]\n",
    "\n",
    "### SOLUTION END\n",
    "\n",
    "    return M, word2Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce a co-occurrence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of number of corpus words)): co-occurrence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"    \n",
    "    np.random.seed(4355)\n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    \n",
    "    ### SOLUTION BEGIN\n",
    "    svd = TruncatedSVD(n_components=k, n_iter=n_iters)\n",
    "    M_reduced = svd.fit_transform(M)\n",
    "    ### SOLUTION END\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return M_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped_words : ['\"', '&', \"'\", '(', ')', '),', ',', ',\"', '-', '.', '.\"', '.)', '/', '0', '000', '04', '08', '09', '1', '10', '100', '106', '13', '15', '16', '165', '178', '1978', '1985', '1986', '1987', '199', '1990', '2', '20', '200', '2000', '21', '23', '248', '25', '26', '27', '28', '3', '30', '300', '31', '335', '35', '371', '38', '39', '4', '40', '42', '5', '511', '520', '54', '550', '555', '56', '561', '57', '58', '6', '600', '63', '65', '7', '700', '71', '73', '741', '76', '8', '9', '90', '92', '98', '99', ';', '<END>', '<START>', '>', '>,', '>.']\n",
      "corpus_index.shape: (10, 1001)\n",
      "0 from 800\n",
      "1 from 800\n",
      "2 from 800\n",
      "3 from 800\n",
      "4 from 800\n",
      "5 from 800\n",
      "6 from 800\n",
      "7 from 800\n",
      "8 from 800\n",
      "9 from 800\n",
      "10 from 800\n",
      "11 from 800\n",
      "12 from 800\n",
      "13 from 800\n",
      "14 from 800\n",
      "15 from 800\n",
      "16 from 800\n",
      "17 from 800\n",
      "18 from 800\n",
      "19 from 800\n",
      "20 from 800\n",
      "21 from 800\n",
      "22 from 800\n",
      "23 from 800\n",
      "24 from 800\n",
      "25 from 800\n",
      "26 from 800\n",
      "27 from 800\n",
      "28 from 800\n",
      "29 from 800\n",
      "30 from 800\n",
      "31 from 800\n",
      "32 from 800\n",
      "33 from 800\n",
      "34 from 800\n",
      "35 from 800\n",
      "36 from 800\n",
      "37 from 800\n",
      "38 from 800\n",
      "39 from 800\n",
      "40 from 800\n",
      "41 from 800\n",
      "42 from 800\n",
      "43 from 800\n",
      "44 from 800\n",
      "45 from 800\n",
      "46 from 800\n",
      "47 from 800\n",
      "48 from 800\n",
      "49 from 800\n",
      "50 from 800\n",
      "51 from 800\n",
      "52 from 800\n",
      "53 from 800\n",
      "54 from 800\n",
      "55 from 800\n",
      "56 from 800\n",
      "57 from 800\n",
      "58 from 800\n",
      "59 from 800\n",
      "60 from 800\n",
      "61 from 800\n",
      "62 from 800\n",
      "63 from 800\n",
      "64 from 800\n",
      "65 from 800\n",
      "66 from 800\n",
      "67 from 800\n",
      "68 from 800\n",
      "69 from 800\n",
      "70 from 800\n",
      "71 from 800\n",
      "72 from 800\n",
      "73 from 800\n",
      "74 from 800\n",
      "75 from 800\n",
      "76 from 800\n",
      "77 from 800\n",
      "78 from 800\n",
      "79 from 800\n",
      "80 from 800\n",
      "81 from 800\n",
      "82 from 800\n",
      "83 from 800\n",
      "84 from 800\n",
      "85 from 800\n",
      "86 from 800\n",
      "87 from 800\n",
      "88 from 800\n",
      "89 from 800\n",
      "90 from 800\n",
      "91 from 800\n",
      "92 from 800\n",
      "93 from 800\n",
      "94 from 800\n",
      "95 from 800\n",
      "96 from 800\n",
      "97 from 800\n",
      "98 from 800\n",
      "99 from 800\n",
      "100 from 800\n",
      "101 from 800\n",
      "102 from 800\n",
      "103 from 800\n",
      "104 from 800\n",
      "105 from 800\n",
      "106 from 800\n",
      "107 from 800\n",
      "108 from 800\n",
      "109 from 800\n",
      "110 from 800\n",
      "111 from 800\n",
      "112 from 800\n",
      "113 from 800\n",
      "114 from 800\n",
      "115 from 800\n",
      "116 from 800\n",
      "117 from 800\n",
      "118 from 800\n",
      "119 from 800\n",
      "120 from 800\n",
      "121 from 800\n",
      "122 from 800\n",
      "123 from 800\n",
      "124 from 800\n",
      "125 from 800\n",
      "126 from 800\n",
      "127 from 800\n",
      "128 from 800\n",
      "129 from 800\n",
      "130 from 800\n",
      "131 from 800\n",
      "132 from 800\n",
      "133 from 800\n",
      "134 from 800\n",
      "135 from 800\n",
      "136 from 800\n",
      "137 from 800\n",
      "138 from 800\n",
      "139 from 800\n",
      "140 from 800\n",
      "141 from 800\n",
      "142 from 800\n",
      "143 from 800\n",
      "144 from 800\n",
      "145 from 800\n",
      "146 from 800\n",
      "147 from 800\n",
      "148 from 800\n",
      "149 from 800\n",
      "150 from 800\n",
      "151 from 800\n",
      "152 from 800\n",
      "153 from 800\n",
      "154 from 800\n",
      "155 from 800\n",
      "156 from 800\n",
      "157 from 800\n",
      "158 from 800\n",
      "159 from 800\n",
      "160 from 800\n",
      "161 from 800\n",
      "162 from 800\n",
      "163 from 800\n",
      "164 from 800\n",
      "165 from 800\n",
      "166 from 800\n",
      "167 from 800\n",
      "168 from 800\n",
      "169 from 800\n",
      "170 from 800\n",
      "171 from 800\n",
      "172 from 800\n",
      "173 from 800\n",
      "174 from 800\n",
      "175 from 800\n",
      "176 from 800\n",
      "177 from 800\n",
      "178 from 800\n",
      "179 from 800\n",
      "180 from 800\n",
      "181 from 800\n",
      "182 from 800\n",
      "183 from 800\n",
      "184 from 800\n",
      "185 from 800\n",
      "186 from 800\n",
      "187 from 800\n",
      "188 from 800\n",
      "189 from 800\n",
      "190 from 800\n",
      "191 from 800\n",
      "192 from 800\n",
      "193 from 800\n",
      "194 from 800\n",
      "195 from 800\n",
      "196 from 800\n",
      "197 from 800\n",
      "198 from 800\n",
      "199 from 800\n",
      "200 from 800\n",
      "201 from 800\n",
      "202 from 800\n",
      "203 from 800\n",
      "204 from 800\n",
      "205 from 800\n",
      "206 from 800\n",
      "207 from 800\n",
      "208 from 800\n",
      "209 from 800\n",
      "210 from 800\n",
      "211 from 800\n",
      "212 from 800\n",
      "213 from 800\n",
      "214 from 800\n",
      "215 from 800\n",
      "216 from 800\n",
      "217 from 800\n",
      "218 from 800\n",
      "219 from 800\n",
      "220 from 800\n",
      "221 from 800\n",
      "222 from 800\n",
      "223 from 800\n",
      "224 from 800\n",
      "225 from 800\n",
      "226 from 800\n",
      "227 from 800\n",
      "228 from 800\n",
      "229 from 800\n",
      "230 from 800\n",
      "231 from 800\n",
      "232 from 800\n",
      "233 from 800\n",
      "234 from 800\n",
      "235 from 800\n",
      "236 from 800\n",
      "237 from 800\n",
      "238 from 800\n",
      "239 from 800\n",
      "240 from 800\n",
      "241 from 800\n",
      "242 from 800\n",
      "243 from 800\n",
      "244 from 800\n",
      "245 from 800\n",
      "246 from 800\n",
      "247 from 800\n",
      "248 from 800\n",
      "249 from 800\n",
      "250 from 800\n",
      "251 from 800\n",
      "252 from 800\n",
      "253 from 800\n",
      "254 from 800\n",
      "255 from 800\n",
      "256 from 800\n",
      "257 from 800\n",
      "258 from 800\n",
      "259 from 800\n",
      "260 from 800\n",
      "261 from 800\n",
      "262 from 800\n",
      "263 from 800\n",
      "264 from 800\n",
      "265 from 800\n",
      "266 from 800\n",
      "267 from 800\n",
      "268 from 800\n",
      "269 from 800\n",
      "270 from 800\n",
      "271 from 800\n",
      "272 from 800\n",
      "273 from 800\n",
      "274 from 800\n",
      "275 from 800\n",
      "276 from 800\n",
      "277 from 800\n",
      "278 from 800\n",
      "279 from 800\n",
      "280 from 800\n",
      "281 from 800\n",
      "282 from 800\n",
      "283 from 800\n",
      "284 from 800\n",
      "285 from 800\n",
      "286 from 800\n",
      "287 from 800\n",
      "288 from 800\n",
      "289 from 800\n",
      "290 from 800\n",
      "291 from 800\n",
      "292 from 800\n",
      "293 from 800\n",
      "294 from 800\n",
      "295 from 800\n",
      "296 from 800\n",
      "297 from 800\n",
      "298 from 800\n",
      "299 from 800\n",
      "300 from 800\n",
      "301 from 800\n",
      "302 from 800\n",
      "303 from 800\n",
      "304 from 800\n",
      "305 from 800\n",
      "306 from 800\n",
      "307 from 800\n",
      "308 from 800\n",
      "309 from 800\n",
      "310 from 800\n",
      "311 from 800\n",
      "312 from 800\n",
      "313 from 800\n",
      "314 from 800\n",
      "315 from 800\n",
      "316 from 800\n",
      "317 from 800\n",
      "318 from 800\n",
      "319 from 800\n",
      "320 from 800\n",
      "321 from 800\n",
      "322 from 800\n",
      "323 from 800\n",
      "324 from 800\n",
      "325 from 800\n",
      "326 from 800\n",
      "327 from 800\n",
      "328 from 800\n",
      "329 from 800\n",
      "330 from 800\n",
      "331 from 800\n",
      "332 from 800\n",
      "333 from 800\n",
      "334 from 800\n",
      "335 from 800\n",
      "336 from 800\n",
      "337 from 800\n",
      "338 from 800\n",
      "339 from 800\n",
      "340 from 800\n",
      "341 from 800\n",
      "342 from 800\n",
      "343 from 800\n",
      "344 from 800\n",
      "345 from 800\n",
      "346 from 800\n",
      "347 from 800\n",
      "348 from 800\n",
      "349 from 800\n",
      "350 from 800\n",
      "351 from 800\n",
      "352 from 800\n",
      "353 from 800\n",
      "354 from 800\n",
      "355 from 800\n",
      "356 from 800\n",
      "357 from 800\n",
      "358 from 800\n",
      "359 from 800\n",
      "360 from 800\n",
      "361 from 800\n",
      "362 from 800\n",
      "363 from 800\n",
      "364 from 800\n",
      "365 from 800\n",
      "366 from 800\n",
      "367 from 800\n",
      "368 from 800\n",
      "369 from 800\n",
      "370 from 800\n",
      "371 from 800\n",
      "372 from 800\n",
      "373 from 800\n",
      "374 from 800\n",
      "375 from 800\n",
      "376 from 800\n",
      "377 from 800\n",
      "378 from 800\n",
      "379 from 800\n",
      "380 from 800\n",
      "381 from 800\n",
      "382 from 800\n",
      "383 from 800\n",
      "384 from 800\n",
      "385 from 800\n",
      "386 from 800\n",
      "387 from 800\n",
      "388 from 800\n",
      "389 from 800\n",
      "390 from 800\n",
      "391 from 800\n",
      "392 from 800\n",
      "393 from 800\n",
      "394 from 800\n",
      "395 from 800\n",
      "396 from 800\n",
      "397 from 800\n",
      "398 from 800\n",
      "399 from 800\n",
      "400 from 800\n",
      "401 from 800\n",
      "402 from 800\n",
      "403 from 800\n",
      "404 from 800\n",
      "405 from 800\n",
      "406 from 800\n",
      "407 from 800\n",
      "408 from 800\n",
      "409 from 800\n",
      "410 from 800\n",
      "411 from 800\n",
      "412 from 800\n",
      "413 from 800\n",
      "414 from 800\n",
      "415 from 800\n",
      "416 from 800\n",
      "417 from 800\n",
      "418 from 800\n",
      "419 from 800\n",
      "420 from 800\n",
      "421 from 800\n",
      "422 from 800\n",
      "423 from 800\n",
      "424 from 800\n",
      "425 from 800\n",
      "426 from 800\n",
      "427 from 800\n",
      "428 from 800\n",
      "429 from 800\n",
      "430 from 800\n",
      "431 from 800\n",
      "432 from 800\n",
      "433 from 800\n",
      "434 from 800\n",
      "435 from 800\n",
      "436 from 800\n",
      "437 from 800\n",
      "438 from 800\n",
      "439 from 800\n",
      "440 from 800\n",
      "441 from 800\n",
      "442 from 800\n",
      "443 from 800\n",
      "444 from 800\n",
      "445 from 800\n",
      "446 from 800\n",
      "447 from 800\n",
      "448 from 800\n",
      "449 from 800\n",
      "450 from 800\n",
      "451 from 800\n",
      "452 from 800\n",
      "453 from 800\n",
      "454 from 800\n",
      "455 from 800\n",
      "456 from 800\n",
      "457 from 800\n",
      "458 from 800\n",
      "459 from 800\n",
      "460 from 800\n",
      "461 from 800\n",
      "462 from 800\n",
      "463 from 800\n",
      "464 from 800\n",
      "465 from 800\n",
      "466 from 800\n",
      "467 from 800\n",
      "468 from 800\n",
      "469 from 800\n",
      "470 from 800\n",
      "471 from 800\n",
      "472 from 800\n",
      "473 from 800\n",
      "474 from 800\n",
      "475 from 800\n",
      "476 from 800\n",
      "477 from 800\n",
      "478 from 800\n",
      "479 from 800\n",
      "480 from 800\n",
      "481 from 800\n",
      "482 from 800\n",
      "483 from 800\n",
      "484 from 800\n",
      "485 from 800\n",
      "486 from 800\n",
      "487 from 800\n",
      "488 from 800\n",
      "489 from 800\n",
      "490 from 800\n",
      "491 from 800\n",
      "492 from 800\n",
      "493 from 800\n",
      "494 from 800\n",
      "495 from 800\n",
      "496 from 800\n",
      "497 from 800\n",
      "498 from 800\n",
      "499 from 800\n",
      "500 from 800\n",
      "501 from 800\n",
      "502 from 800\n",
      "503 from 800\n",
      "504 from 800\n",
      "505 from 800\n",
      "506 from 800\n",
      "507 from 800\n",
      "508 from 800\n",
      "509 from 800\n",
      "510 from 800\n",
      "511 from 800\n",
      "512 from 800\n",
      "513 from 800\n",
      "514 from 800\n",
      "515 from 800\n",
      "516 from 800\n",
      "517 from 800\n",
      "518 from 800\n",
      "519 from 800\n",
      "520 from 800\n",
      "521 from 800\n",
      "522 from 800\n",
      "523 from 800\n",
      "524 from 800\n",
      "525 from 800\n",
      "526 from 800\n",
      "527 from 800\n",
      "528 from 800\n",
      "529 from 800\n",
      "530 from 800\n",
      "531 from 800\n",
      "532 from 800\n",
      "533 from 800\n",
      "534 from 800\n",
      "535 from 800\n",
      "536 from 800\n",
      "537 from 800\n",
      "538 from 800\n",
      "539 from 800\n",
      "540 from 800\n",
      "541 from 800\n",
      "542 from 800\n",
      "543 from 800\n",
      "544 from 800\n",
      "545 from 800\n",
      "546 from 800\n",
      "547 from 800\n",
      "548 from 800\n",
      "549 from 800\n",
      "550 from 800\n",
      "551 from 800\n",
      "552 from 800\n",
      "553 from 800\n",
      "554 from 800\n",
      "555 from 800\n",
      "556 from 800\n",
      "557 from 800\n",
      "558 from 800\n",
      "559 from 800\n",
      "560 from 800\n",
      "561 from 800\n",
      "562 from 800\n",
      "563 from 800\n",
      "564 from 800\n",
      "565 from 800\n",
      "566 from 800\n",
      "567 from 800\n",
      "568 from 800\n",
      "569 from 800\n",
      "570 from 800\n",
      "571 from 800\n",
      "572 from 800\n",
      "573 from 800\n",
      "574 from 800\n",
      "575 from 800\n",
      "576 from 800\n",
      "577 from 800\n",
      "578 from 800\n",
      "579 from 800\n",
      "580 from 800\n",
      "581 from 800\n",
      "582 from 800\n",
      "583 from 800\n",
      "584 from 800\n",
      "585 from 800\n",
      "586 from 800\n",
      "587 from 800\n",
      "588 from 800\n",
      "589 from 800\n",
      "590 from 800\n",
      "591 from 800\n",
      "592 from 800\n",
      "593 from 800\n",
      "594 from 800\n",
      "595 from 800\n",
      "596 from 800\n",
      "597 from 800\n",
      "598 from 800\n",
      "599 from 800\n",
      "600 from 800\n",
      "601 from 800\n",
      "602 from 800\n",
      "603 from 800\n",
      "604 from 800\n",
      "605 from 800\n",
      "606 from 800\n",
      "607 from 800\n",
      "608 from 800\n",
      "609 from 800\n",
      "610 from 800\n",
      "611 from 800\n",
      "612 from 800\n",
      "613 from 800\n",
      "614 from 800\n",
      "615 from 800\n",
      "616 from 800\n",
      "617 from 800\n",
      "618 from 800\n",
      "619 from 800\n",
      "620 from 800\n",
      "621 from 800\n",
      "622 from 800\n",
      "623 from 800\n",
      "624 from 800\n",
      "625 from 800\n",
      "626 from 800\n",
      "627 from 800\n",
      "628 from 800\n",
      "629 from 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630 from 800\n",
      "631 from 800\n",
      "632 from 800\n",
      "633 from 800\n",
      "634 from 800\n",
      "635 from 800\n",
      "636 from 800\n",
      "637 from 800\n",
      "638 from 800\n",
      "639 from 800\n",
      "640 from 800\n",
      "641 from 800\n",
      "642 from 800\n",
      "643 from 800\n",
      "644 from 800\n",
      "645 from 800\n",
      "646 from 800\n",
      "647 from 800\n",
      "648 from 800\n",
      "649 from 800\n",
      "650 from 800\n",
      "651 from 800\n",
      "652 from 800\n",
      "653 from 800\n",
      "654 from 800\n",
      "655 from 800\n",
      "656 from 800\n",
      "657 from 800\n",
      "658 from 800\n",
      "659 from 800\n",
      "660 from 800\n",
      "661 from 800\n",
      "662 from 800\n",
      "663 from 800\n",
      "664 from 800\n",
      "665 from 800\n",
      "666 from 800\n",
      "667 from 800\n",
      "668 from 800\n",
      "669 from 800\n",
      "670 from 800\n",
      "671 from 800\n",
      "672 from 800\n",
      "673 from 800\n",
      "674 from 800\n",
      "675 from 800\n",
      "676 from 800\n",
      "677 from 800\n",
      "678 from 800\n",
      "679 from 800\n",
      "680 from 800\n",
      "681 from 800\n",
      "682 from 800\n",
      "683 from 800\n",
      "684 from 800\n",
      "685 from 800\n",
      "686 from 800\n",
      "687 from 800\n",
      "688 from 800\n",
      "689 from 800\n",
      "690 from 800\n",
      "691 from 800\n",
      "692 from 800\n",
      "693 from 800\n",
      "694 from 800\n",
      "695 from 800\n",
      "696 from 800\n",
      "697 from 800\n",
      "698 from 800\n",
      "699 from 800\n",
      "700 from 800\n",
      "701 from 800\n",
      "702 from 800\n",
      "703 from 800\n",
      "704 from 800\n",
      "705 from 800\n",
      "706 from 800\n",
      "707 from 800\n",
      "708 from 800\n",
      "709 from 800\n",
      "710 from 800\n",
      "711 from 800\n",
      "712 from 800\n",
      "713 from 800\n",
      "714 from 800\n",
      "715 from 800\n",
      "716 from 800\n",
      "717 from 800\n",
      "718 from 800\n",
      "719 from 800\n",
      "720 from 800\n",
      "721 from 800\n",
      "722 from 800\n",
      "723 from 800\n",
      "724 from 800\n",
      "725 from 800\n",
      "726 from 800\n",
      "727 from 800\n",
      "728 from 800\n",
      "729 from 800\n",
      "730 from 800\n",
      "731 from 800\n",
      "732 from 800\n",
      "733 from 800\n",
      "734 from 800\n",
      "735 from 800\n",
      "736 from 800\n",
      "737 from 800\n",
      "738 from 800\n",
      "739 from 800\n",
      "740 from 800\n",
      "741 from 800\n",
      "742 from 800\n",
      "743 from 800\n",
      "744 from 800\n",
      "745 from 800\n",
      "746 from 800\n",
      "747 from 800\n",
      "748 from 800\n",
      "749 from 800\n",
      "750 from 800\n",
      "751 from 800\n",
      "752 from 800\n",
      "753 from 800\n",
      "754 from 800\n",
      "755 from 800\n",
      "756 from 800\n",
      "757 from 800\n",
      "758 from 800\n",
      "759 from 800\n",
      "760 from 800\n",
      "761 from 800\n",
      "762 from 800\n",
      "763 from 800\n",
      "764 from 800\n",
      "765 from 800\n",
      "766 from 800\n",
      "767 from 800\n",
      "768 from 800\n",
      "769 from 800\n",
      "770 from 800\n",
      "771 from 800\n",
      "772 from 800\n",
      "773 from 800\n",
      "774 from 800\n",
      "775 from 800\n",
      "776 from 800\n",
      "777 from 800\n",
      "778 from 800\n",
      "779 from 800\n",
      "780 from 800\n",
      "781 from 800\n",
      "782 from 800\n",
      "783 from 800\n",
      "784 from 800\n",
      "785 from 800\n",
      "786 from 800\n",
      "787 from 800\n",
      "788 from 800\n",
      "789 from 800\n",
      "790 from 800\n",
      "791 from 800\n",
      "792 from 800\n",
      "793 from 800\n",
      "794 from 800\n",
      "795 from 800\n",
      "796 from 800\n",
      "797 from 800\n",
      "798 from 800\n",
      "799 from 800\n",
      "Result Matrix Count: 12726\n"
     ]
    }
   ],
   "source": [
    "M_co_occurrence, word2Ind_co_occurrence = compute_co_occurrence_matrix(reuters_corpus[:10])\n",
    "cnt = np.count_nonzero(M_co_occurrence > 0)\n",
    "if np.count_nonzero(M_co_occurrence > 0) == 0:\n",
    "    raise Exception(\"Matrix's empty\")\n",
    "print(f\"Result Matrix Count: {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., 39.,  5., ...,  1.,  0.,  1.],\n",
       "       [39.,  0.,  1., ...,  0.,  1.,  1.],\n",
       "       [ 5.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_co_occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Truncated SVD over 800 words...\n",
      "Done.\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "M_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, k=2)\n",
    "print(M_reduced_co_occurrence.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
    "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(M_reduced, word2Ind, words, title):\n",
    "    plt.clf()\n",
    "    for word in words:\n",
    "        idx = word2Ind[word]\n",
    "        x = M_reduced[idx, 0]\n",
    "        y = M_reduced[idx, 1]\n",
    "        plt.scatter(x, y, marker='x', color='red')\n",
    "        plt.text(x, y, word, fontsize=9)\n",
    "    plt.savefig(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_normalized.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_reduced_co_occurrence.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ecuador'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-327-8ed0ede18751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# words_list = ['barrels', 'bpd']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplot_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2Ind_co_occurrence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'co_occurrence_embeddings5.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-323-1e52d96e6504>\u001b[0m in \u001b[0;36mplot_embeddings\u001b[0;34m(M_reduced, word2Ind, words, title)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2Ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM_reduced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM_reduced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ecuador'"
     ]
    }
   ],
   "source": [
    "# words_list = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\n",
    "words_list = ['barrels', 'energy', 'industry', 'oil', 'petroleum']\n",
    "# words_list = ['an', 'year']\n",
    "# words_list = ['barrels', 'bpd']\n",
    "\n",
    "plot_embeddings(M_normalized, word2Ind_co_occurrence, words_list, 'co_occurrence_embeddings5.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc only diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store doc_index as max words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2pac bug\n",
    "# do not filter out tokens that starts from number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
